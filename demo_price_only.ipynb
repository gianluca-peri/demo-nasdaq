{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gianluca-peri/demo-nasdaq/blob/main/demo_price_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjLu2ZszlJ7z"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "filename = 'nq-12-24.txt'\n",
        "id = '11qqmeXzP_jYcuYsJ382TP6M2dTuD5HDm'\n",
        "\n",
        "replace = False\n",
        "\n",
        "if replace:\n",
        "  if os.path.exists(filename):\n",
        "    os.remove(filename)\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  # Use gdown to download the file by its ID\n",
        "  gdown.download(f'https://drive.google.com/uc?id={id}', filename)\n",
        "\n",
        "# Reading\n",
        "with open(filename, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Print the size of it in MB\n",
        "print('\\nSize of file:')\n",
        "size_in_mb = round(os.path.getsize(filename) / (1024 * 1024), 2)\n",
        "print(f'{size_in_mb:.2f} MB')\n",
        "\n",
        "# Print the head of it\n",
        "print('\\nHead of file:')\n",
        "print(content[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua2heM0Ytz_Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(f'Device: {device}!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxLt0vFA60no"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reading it as a pd dataframe\n",
        "\n",
        "data = pd.read_csv('nq-12-24.txt', sep=\";\", header=None)\n",
        "data.columns = [\"timestamp\", \"trade_price\", \"best_bid\", \"best_ask\", \"volume\"]\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAlmSfaH78E_"
      },
      "outputs": [],
      "source": [
        "# Turning the timestamp into a proper datetime object\n",
        "\n",
        "data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y%m%d %H%M%S %f')\n",
        "data.rename(columns={'timestamp': 'datetime'}, inplace=True)\n",
        "\n",
        "# Turn from italian into Easter Time\n",
        "\n",
        "data['datetime'] = data['datetime'].dt.tz_localize('Europe/Rome').dt.tz_convert('US/Eastern')\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLbMnAE_8-Ip"
      },
      "outputs": [],
      "source": [
        "# We should check for presence of NaN values\n",
        "\n",
        "for column in data.columns:\n",
        "    print(f'Column {column} has {data[column].isna().sum()} NaN values\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hThEH_K19M8i"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Making a graph of trade price vs time\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Select only one in a 1000 to plot\n",
        "datetimes_plot = data['datetime'][::1000]\n",
        "trade_prices_plot = data['trade_price'][::1000]\n",
        "\n",
        "# Plot trade price\n",
        "plt.scatter(datetimes_plot, trade_prices_plot, s=0.5, label='Trade price')\n",
        "\n",
        "# Fill in green workdays\n",
        "\n",
        "plt.fill_between(\n",
        "    datetimes_plot,\n",
        "    trade_prices_plot.min(),\n",
        "    trade_prices_plot.max(),\n",
        "    where=(datetimes_plot.dt.weekday < 5),\n",
        "    color='green',\n",
        "    alpha=0.2,\n",
        "    label='Work days'\n",
        "  )\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Trade price (points)')\n",
        "plt.title('Trade price vs time')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIuEDShXv8g-"
      },
      "outputs": [],
      "source": [
        "# To clean the code we need to:\n",
        "# Remove the weekends if present\n",
        "# Remove the non standard trading periods\n",
        "\n",
        "time_delta = pd.Timedelta(minutes=10)\n",
        "start_time = data['datetime'][0]\n",
        "\n",
        "# Create helper column\n",
        "data['time_slot'] = (data['datetime'] - start_time) // time_delta\n",
        "\n",
        "time_slot_counts = data['time_slot'].value_counts().sort_index()\n",
        "time_slot_datetime = data.groupby('time_slot')['datetime'].first()\n",
        "\n",
        "# Remove helper column\n",
        "data.drop(columns=['time_slot'], inplace=True)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.bar(\n",
        "    x = time_slot_datetime,\n",
        "    height = time_slot_counts,\n",
        "    width = time_delta,\n",
        "    label = 'Number of entries'\n",
        "  )\n",
        "\n",
        "# Fill in green data to keep\n",
        "# (Normal trading is between 9/9:30 and 16:00)\n",
        "\n",
        "plt.fill_between(\n",
        "    time_slot_datetime,\n",
        "    0,\n",
        "    time_slot_counts.max(),\n",
        "    where=((time_slot_datetime.dt.weekday < 5) & (time_slot_datetime.dt.hour >= 9) & (time_slot_datetime.dt.hour <= 16)),\n",
        "    color='green',\n",
        "    alpha=0.2,\n",
        "    label='Normal trading hours'\n",
        "  )\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of entries')\n",
        "plt.title('Number of entries per time slot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkUGqBnfKGvD"
      },
      "outputs": [],
      "source": [
        "# From the pandas dataset drop all the rows outside the normal trading hours\n",
        "# (So get only the green parts)\n",
        "\n",
        "data = data[(data['datetime'].dt.weekday < 5) & (data['datetime'].dt.hour >= 9) & (data['datetime'].dt.hour <= 16)]\n",
        "\n",
        "# Make a plot\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Select only one in a 1000 to plot\n",
        "datetimes_plot = data['datetime'][::100]\n",
        "trade_prices_plot = data['trade_price'][::100]\n",
        "\n",
        "plt.scatter(datetimes_plot, trade_prices_plot, s=0.5, label='Trade price')\n",
        "\n",
        "plt.title('Trade price vs time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Trade price (points)')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLBNV2G9Mk6p"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "# Create a list of timeseries with the trade prices, one for each day\n",
        "\n",
        "do_only_the_first_4_days = True # 4 because the first is too short and will be eliminated\n",
        "\n",
        "trade_prices_list = []\n",
        "datetimes_list = []\n",
        "\n",
        "for day in tqdm(data['datetime'].dt.date.unique(), desc='Building time series'):\n",
        "    trade_prices_list.append(data[data['datetime'].dt.date == day]['trade_price'].tolist())\n",
        "    datetimes_list.append(data[data['datetime'].dt.date == day]['datetime'].tolist())\n",
        "    if do_only_the_first_4_days:\n",
        "        if len(trade_prices_list) == 4:\n",
        "            break\n",
        "\n",
        "# Print lenght of lists\n",
        "\n",
        "for index, a_list in enumerate(trade_prices_list):\n",
        "    print(f'Lenght of the list for day {index}: {len(a_list)}')\n",
        "\n",
        "# Get mean lenght\n",
        "\n",
        "mean_lenght = sum([len(a_list) for a_list in trade_prices_list]) / len(trade_prices_list)\n",
        "print(f'\\nMean lenght: {mean_lenght}\\n')\n",
        "\n",
        "# Drop short outliers\n",
        "\n",
        "trade_prices_list = [a_list for a_list in trade_prices_list if len(a_list) > mean_lenght*0.1]\n",
        "datetimes_list = [a_list for a_list in datetimes_list if len(a_list) > mean_lenght*0.1]\n",
        "\n",
        "# Print lenght of lists again\n",
        "\n",
        "for index, a_list in enumerate(trade_prices_list):\n",
        "    print(f'Lenght of the list for day {index}: {len(a_list)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bWqgZMhiWOE"
      },
      "outputs": [],
      "source": [
        "# Plot each time series by itself in a multiplot\n",
        "\n",
        "fig, axs = plt.subplots(len(trade_prices_list), 1, figsize=(15, 5*len(trade_prices_list)))\n",
        "\n",
        "for index in range(len(trade_prices_list)):\n",
        "    axs[index].scatter(datetimes_list[index][::100], trade_prices_list[index][::100], s=0.5, label='Trade price')\n",
        "    axs[index].set_title(f'Time series {index}')\n",
        "    axs[index].set_xlabel('Time')\n",
        "    axs[index].set_ylabel('Trade price (points)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1dtn18UOnmS"
      },
      "outputs": [],
      "source": [
        "# Let's build a data dictionary for each day\n",
        "\n",
        "data_dicts_list = []\n",
        "\n",
        "for a_list in trade_prices_list:\n",
        "    data_dicts_list.append({\n",
        "        'x' : [i for i in range(len(a_list))],\n",
        "        'y' : a_list\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbZ0W93_Q5_-"
      },
      "outputs": [],
      "source": [
        "# We can use this, with pytorch, to create a custom dataset for classification\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleTimeSeriesDataset(Dataset):\n",
        "  def __init__(self, time_series, input_len, future_lookup_len, delta, stride, normalize):\n",
        "\n",
        "    self.time_series = time_series\n",
        "    self.input_len = input_len\n",
        "    self.future_lookup_len = future_lookup_len\n",
        "    self.delta = delta\n",
        "    self.stride = stride\n",
        "    self.normalize = normalize\n",
        "    self.min_price = min(time_series['y'])\n",
        "    self.max_price = max(time_series['y'])\n",
        "\n",
        "    if normalize:\n",
        "      self.time_series['y'] = [(y - self.min_price) / (self.max_price - self.min_price) for y in time_series['y']]\n",
        "      self.delta = self.delta / (self.max_price - self.min_price)\n",
        "\n",
        "    self.lenght_of_time_series = len(time_series['x'])\n",
        "    self.slices = []\n",
        "\n",
        "    points_since_last_save = 0\n",
        "    for i in tqdm(range(self.lenght_of_time_series - input_len - future_lookup_len), desc='Building the classification dataset...'):\n",
        "      if points_since_last_save > self.stride:\n",
        "        self.slices.append({\n",
        "            'x': time_series['x'][i:i+input_len+future_lookup_len],\n",
        "            'y': time_series['y'][i:i+input_len+future_lookup_len]\n",
        "        })\n",
        "        points_since_last_save = 0\n",
        "      else:\n",
        "        points_since_last_save += 1\n",
        "\n",
        "  def classify_time_series(self, y_past, y_future):\n",
        "    last_known_y = y_past[-1]\n",
        "    floor = last_known_y - self.delta\n",
        "    ceiling = last_known_y + self.delta\n",
        "\n",
        "    for point in y_future:\n",
        "      if point > ceiling:\n",
        "        # goes up\n",
        "        return torch.tensor(0)\n",
        "      elif point < floor:\n",
        "        # goes down\n",
        "        return torch.tensor(1)\n",
        "\n",
        "    # neutral\n",
        "    return torch.tensor(2)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.slices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    current_slice = self.slices[idx]\n",
        "\n",
        "    y_past = current_slice['y'][:self.input_len]\n",
        "    y_future = current_slice['y'][self.input_len:]\n",
        "\n",
        "    input_features_vector = torch.Tensor(y_past)\n",
        "    correct_output = self.classify_time_series(y_past, y_future)\n",
        "\n",
        "    return input_features_vector, correct_output\n",
        "\n",
        "# Use 3 days to get train, validation, and test\n",
        "\n",
        "TIME_DELTA_IN_DATAPOINTS = 1000\n",
        "STRIDE = 100\n",
        "DELTA = 10\n",
        "\n",
        "train_dataset = SimpleTimeSeriesDataset(data_dicts_list[0], TIME_DELTA_IN_DATAPOINTS, TIME_DELTA_IN_DATAPOINTS, DELTA, STRIDE, normalize=True)\n",
        "validation_dataset = SimpleTimeSeriesDataset(data_dicts_list[1], TIME_DELTA_IN_DATAPOINTS, TIME_DELTA_IN_DATAPOINTS, DELTA, STRIDE, normalize=True)\n",
        "test_dataset = SimpleTimeSeriesDataset(data_dicts_list[2], TIME_DELTA_IN_DATAPOINTS, TIME_DELTA_IN_DATAPOINTS, DELTA, STRIDE, normalize=True)\n",
        "\n",
        "# Print lenght\n",
        "\n",
        "print(f'Lenght of train dataset: {len(train_dataset)}')\n",
        "print(f'Lenght of validation dataset: {len(validation_dataset)}')\n",
        "print(f'Lenght of test dataset: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxELhxzDwHlg"
      },
      "outputs": [],
      "source": [
        "print('Checking train dataset\\'s composition...')\n",
        "\n",
        "number_of_goes_up_classes = 0\n",
        "number_of_goes_down_classes = 0\n",
        "number_of_neutral_classes = 0\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "  input_features_vector, correct_output = train_dataset[i]\n",
        "  if correct_output.item() == 0:\n",
        "    number_of_goes_up_classes += 1\n",
        "  elif correct_output.item() == 1:\n",
        "    number_of_goes_down_classes += 1\n",
        "  elif correct_output.item() == 2:\n",
        "    number_of_neutral_classes += 1\n",
        "  else:\n",
        "    raise Exception('Unknown class')\n",
        "\n",
        "print(f'Number of goes up classes in train: {number_of_goes_up_classes}')\n",
        "print(f'Number of goes down classes in train: {number_of_goes_down_classes}')\n",
        "print(f'Number of neutral classes in train: {number_of_neutral_classes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KhWRHuesSFA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Plot examples of the custom dataset\n",
        "\n",
        "dataset = train_dataset\n",
        "\n",
        "fig, ax = plt.subplots(2, 3, figsize=(20,10))\n",
        "\n",
        "fig.suptitle('Examples of the custom dataset', fontsize=16)\n",
        "\n",
        "for i in range(2):\n",
        "  for j in range(3):\n",
        "    k = np.random.randint(0, len(dataset))\n",
        "    x_slice = dataset.slices[k]['x']\n",
        "    y_slice = dataset.slices[k]['y']\n",
        "    x_past = x_slice[:TIME_DELTA_IN_DATAPOINTS]\n",
        "    y_past = y_slice[:TIME_DELTA_IN_DATAPOINTS]\n",
        "    x_future = x_slice[TIME_DELTA_IN_DATAPOINTS:]\n",
        "    y_future = y_slice[TIME_DELTA_IN_DATAPOINTS:]\n",
        "    last_known_y = y_past[-1]\n",
        "    floor = last_known_y - train_dataset.delta\n",
        "    ceiling = last_known_y + train_dataset.delta\n",
        "    ax[i][j].scatter(x_past, y_past, color='blue', s=0.5, label='past')\n",
        "    ax[i][j].scatter(x_future, y_future, color='red', s=0.5, label='future')\n",
        "    ax[i][j].axhline(y=last_known_y, color='black', linestyle='--', label='current value')\n",
        "    ax[i][j].axhline(y=floor, color='orange', linestyle='--', label='floor')\n",
        "    ax[i][j].axhline(y=ceiling, color='green', linestyle='--', label='ceiling')\n",
        "    ax[i][j].legend()\n",
        "    ax[i][j].set_xlabel('x')\n",
        "    ax[i][j].set_ylabel('y')\n",
        "    ax[i][j].tick_params(axis='x', labelsize=8)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbswli4ypNwL"
      },
      "source": [
        "# Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTPzcoE_s2Hz"
      },
      "outputs": [],
      "source": [
        "!pip3 install torchview 1> /dev/null 2> /dev/null\n",
        "from torchview import draw_graph\n",
        "import matplotlib.image as mpimg\n",
        "from torch import nn\n",
        "\n",
        "# Now we can try to classify using a simple MLP\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# MLP definition\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sequential = torch.nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_size),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.sequential(x)\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "input_size = TIME_DELTA_IN_DATAPOINTS\n",
        "hidden_size = 1000\n",
        "output_size = 3\n",
        "\n",
        "model = MLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Visualize the network\n",
        "\n",
        "graph = draw_graph(model, input_size=(1, input_size)) # (batch_size, real_input_size)\n",
        "\n",
        "# Render to PNG file\n",
        "output_path = \"model_graph\"\n",
        "graph.visual_graph.render(filename=output_path, format=\"png\", cleanup=True)\n",
        "\n",
        "# Load the image and display with matplotlib\n",
        "img = mpimg.imread(f\"{output_path}.png\")\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqaasEd9pSGd"
      },
      "source": [
        "# Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-pTgKJOtMwW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "losses_train = []\n",
        "losses_validation = []\n",
        "\n",
        "# Use GPU\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(train_dataloader):\n",
        "    input_features_vector = input_features_vector.to(device)\n",
        "    label = label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_features_vector)\n",
        "    loss = loss_fn(output, label)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  losses_train.append(total_loss/len(train_dataloader))\n",
        "\n",
        "  # Check validation\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (input_features_vector, label) in enumerate(validation_dataloader):\n",
        "      input_features_vector = input_features_vector.to(device)\n",
        "      label = label.to(device)\n",
        "      output = model(input_features_vector)\n",
        "      loss = loss_fn(output, label)\n",
        "      total_loss += loss.item()\n",
        "    losses_validation.append(total_loss/len(validation_dataloader))\n",
        "\n",
        "# Making a plot of loss vs epochs\n",
        "\n",
        "plt.plot(losses_train, label='train')\n",
        "plt.plot(losses_validation, label='validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model to get the train accuracy\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(train_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.argmax(output[i]) == label[i]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on train: {correct/total}')\n"
      ],
      "metadata": {
        "id": "Gz99Zt1wQABz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNtn0J0sQGBT"
      },
      "outputs": [],
      "source": [
        "# Test the model to get the accuracy\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(test_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.argmax(output[i]) == label[i]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on test: {correct/total}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNvwzUqaQKHj"
      },
      "outputs": [],
      "source": [
        "# Test again with a confidence level\n",
        "\n",
        "confidence = 0.8\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "wrong = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(test_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.max(output[i]) >= confidence:\n",
        "        if torch.argmax(output[i]) == label[i]:\n",
        "          correct += 1\n",
        "        else:\n",
        "          wrong += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on test with confidence {confidence}: {correct/(correct+wrong)}')\n",
        "print(f'Activity on test with confidence {confidence}: {(correct+wrong)/total}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do the same thing but with a CNN\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, kernel_number=16, kernel_size=100, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv1d(1, kernel_number, kernel_size, stride, padding),  # [B, 16, L]\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.flattened_size = kernel_number * ((input_size + 2 * padding - kernel_size) // stride + 1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [B, 1, L]\n",
        "        x = self.conv_net(x)         # [B, 16, L]\n",
        "        x = x.view(x.size(0), -1)    # Flatten: [B, 16 * L]\n",
        "        logits = self.fc(x)          # [B, 3]\n",
        "        return logits\n",
        "\n",
        "model = SimpleCNN(TIME_DELTA_IN_DATAPOINTS, hidden_size, output_size)\n",
        "\n",
        "# Visualize the network\n",
        "\n",
        "graph = draw_graph(model, input_size=(1,input_size))\n",
        "\n",
        "# Render to PNG file\n",
        "output_path = \"model_graph\"\n",
        "graph.visual_graph.render(filename=output_path, format=\"png\", cleanup=True)\n",
        "\n",
        "# Load the image and display with matplotlib\n",
        "img = mpimg.imread(f\"{output_path}.png\")\n",
        "plt.figure(figsize=(6,20))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ecYOMbRSWoaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "losses_train = []\n",
        "losses_validation = []\n",
        "\n",
        "# Use GPU\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(train_dataloader):\n",
        "    input_features_vector = input_features_vector.to(device)\n",
        "    label = label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_features_vector)\n",
        "    loss = loss_fn(output, label)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  losses_train.append(total_loss/len(train_dataloader))\n",
        "\n",
        "  # Check validation\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (input_features_vector, label) in enumerate(validation_dataloader):\n",
        "      input_features_vector = input_features_vector.to(device)\n",
        "      label = label.to(device)\n",
        "      output = model(input_features_vector)\n",
        "      loss = loss_fn(output, label)\n",
        "      total_loss += loss.item()\n",
        "    losses_validation.append(total_loss/len(validation_dataloader))\n",
        "\n",
        "# Making a plot of loss vs epochs\n",
        "\n",
        "plt.plot(losses_train, label='train')\n",
        "plt.plot(losses_validation, label='validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZH4tCU3zZrag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model to get the train accuracy\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(train_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.argmax(output[i]) == label[i]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on train: {correct/total}')\n"
      ],
      "metadata": {
        "id": "OpasF5rO-Tom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ-drEtp-bX1"
      },
      "outputs": [],
      "source": [
        "# Test the model to get the accuracy\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(test_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.argmax(output[i]) == label[i]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on test: {correct/total}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Yo0vmH-fQo"
      },
      "outputs": [],
      "source": [
        "# Test again with a confidence level\n",
        "\n",
        "confidence = 0.8\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "wrong = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (input_features_vector, label) in enumerate(test_dataloader):\n",
        "    output = model(input_features_vector)\n",
        "    for i in range(len(output)):\n",
        "      if torch.max(output[i]) >= confidence:\n",
        "        if torch.argmax(output[i]) == label[i]:\n",
        "          correct += 1\n",
        "        else:\n",
        "          wrong += 1\n",
        "      total += 1\n",
        "\n",
        "print(f'Accuracy on test with confidence {confidence}: {correct/(correct+wrong)}')\n",
        "print(f'Activity on test with confidence {confidence}: {(correct+wrong)/total}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOrK844WSxh3bzUqXXYE4Mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}